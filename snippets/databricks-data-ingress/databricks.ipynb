{"cells":[{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"a44cb797-1373-4bb1-8643-256a2578016c","showTitle":false,"title":""}},"source":["#Data Hub to Spark Dataframe Example"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"cecbeb65-6098-4d5b-8edc-ef511e9aa1e6","showTitle":false,"title":""}},"source":["##Install Data Hub Python Library"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"48b0b403-4745-45d6-8d43-8c12c9dbe6a0","showTitle":false,"title":""}},"outputs":[],"source":["!python -m pip install --upgrade pip\n","!python -m pip install  adh_sample_library_preview --upgrade"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"6195e839-176c-4511-ab69-8c8ffbe77efc","showTitle":false,"title":""}},"source":["##Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"819fcc3e-bcad-4793-9cdf-87818c91f3df","showTitle":false,"title":""}},"outputs":[],"source":["from adh_sample_library_preview import ADHClient\n","from concurrent.futures import ThreadPoolExecutor\n","from datetime import timedelta\n","from itertools import repeat\n","import json\n","import os\n","import pandas as pd\n","import time"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"384aab3d-b768-4adc-99db-da169962fe2a","showTitle":false,"title":""}},"source":["##Create a Data Hub client and specify data to pull"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"c7369079-0c25-4c34-9c84-80a78f455120","showTitle":false,"title":""}},"outputs":[],"source":["client = ADHClient(\n","  'v1',\n","  dbutils.secrets.get(scope = \"key-vault\", key = \"DatabricksTenantId\"),\n","  'https://uswe.datahub.connect.aveva.com',\n","  dbutils.secrets.get(scope = \"key-vault\", key = \"DatabricksClientId\"),\n","  dbutils.secrets.get(scope = \"key-vault\", key = \"DatabricksclientSecret\"))\n","namespace_id = dbutils.secrets.get(scope = \"key-vault\", key = \"DatabricksNamespaceId\")\n","\n","client.request_timeout = 3000\n","\n","data_view_id = 'Wind Turbine Analysis'\n","start_index = '2022-04-01T00:00:00.000Z'\n","end_index = '2022-05-01T00:00:00.000Z'\n","interval = timedelta(minutes = 1)"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"87c65967-eff0-40ba-8a70-2dfbff40aaa9","showTitle":false,"title":""}},"source":["##Write function to pull data from Data View and generate a Spark Data Frame"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"53846325-9ba8-4f53-a5cf-ad1ea16d9279","showTitle":false,"title":""}},"outputs":[],"source":["def retryWithBackoff(fn, *args, **kwargs):\n","  result = None\n","  failures = 0\n","  while result is None:\n","    try:\n","      result = fn(*args, **kwargs)\n","    except Exception as error:\n","      print('request failed retrying...')\n","      if (2^failures-1) < 3600:\n","        time.sleep(2 ** failures)\n","        failures += 1\n","      else:\n","        time.sleep(3600)\n","  \n","  return result\n","\n","def dataViewRequest(client, namespace_id, data_view_id, start_index, end_index, interval, cache):\n","  data_page, next_page, first_page = retryWithBackoff(client.DataViews.getDataInterpolated, namespace_id, data_view_id, start_index=start_index, end_index=end_index, interval = interval, cache = cache)\n","  df = spark.read.json(sc.parallelize([json.dumps(data_page)]))\n","\n","  # iterate through each subsequent page of results until there are no more pages\n","  while next_page:\n","    data_page, next_page, first_page = retryWithBackoff(client.DataViews.getDataInterpolated, url = next_page)\n","    df = df.union(spark.read.json(sc.parallelize([json.dumps(data_page)])))\n","    \n","  return df"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"985411a4-f055-4977-8db6-b6d332d6b1a1","showTitle":false,"title":""}},"source":["##Simple data request"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"19f88038-1995-4af9-81a5-23d506f9cbba","showTitle":false,"title":""}},"outputs":[],"source":["df = dataViewRequest(client, namespace_id, data_view_id, start_index, end_index, interval, 'Refresh')\n","print(df.count())\n","df.show()"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"b92cc356-010e-4ef7-a8db-7f883bc8894e","showTitle":false,"title":""}},"source":["##Parallel data request"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"03349bdf-adfc-4dde-8484-b9f331392ac0","showTitle":false,"title":""}},"outputs":[],"source":["# split time range from start_index to end_index into a list of intervals equal to the number of parallel executors\n","number_of_executors = min(32, os.cpu_count() + 4)\n","\n","# split time range from start_index to end_index into a list of intervals equal to the number of parallel executors\n","intervals = pd.date_range(start=start_index, end=end_index, freq=interval)\n","intervals = intervals[0::round(len(intervals)/(number_of_executors+1))]\n","print(intervals)\n","\n","# convert the list of intervals to a list of start_indexes and end_indexes\n","start_indexes = intervals[:-1].tolist()\n","start_indexes = [i.strftime(\"%Y-%m-%dT%H:%M:%S\") for i in start_indexes]\n","end_indexes = intervals[1:].tolist()\n","end_indexes = [(i - interval).strftime(\"%Y-%m-%dT%H:%M:%S\") for i in end_indexes]\n","end_indexes[-1] = end_index\n","\n","with ThreadPoolExecutor(max_workers=number_of_executors) as pool:\n","  # for each start_index and end_index, start a thread to pull data into a data_frame\n","  data_frames = pool.map(dataViewRequest, repeat(client), repeat(namespace_id), repeat(data_view_id), start_indexes, end_indexes, repeat(interval), repeat('Refresh'))\n","  \n","  # combine all the data_frames into one\n","  df = next(data_frames)\n","  for data_frame in data_frames:\n","    df = df.union(data_frame)\n","    \n","print(df.count())\n","df.show()"]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"pythonIndentUnit":2},"notebookName":"Data Hub Data","notebookOrigID":1613947921670944,"widgets":{}},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
