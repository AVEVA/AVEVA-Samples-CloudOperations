{"cells":[{"cell_type":"markdown","source":["#Data Hub to Spark Dataframe Example"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a44cb797-1373-4bb1-8643-256a2578016c"}}},{"cell_type":"markdown","source":["##Install Data Hub Python Library"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cecbeb65-6098-4d5b-8edc-ef511e9aa1e6"}}},{"cell_type":"code","source":["!python -m pip install --upgrade pip\n!python -m pip install  adh_sample_library_preview --upgrade"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"48b0b403-4745-45d6-8d43-8c12c9dbe6a0"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##Imports"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6195e839-176c-4511-ab69-8c8ffbe77efc"}}},{"cell_type":"code","source":["from adh_sample_library_preview import ADHClient\nfrom concurrent.futures import ThreadPoolExecutor\nfrom datetime import timedelta\nfrom itertools import repeat\nimport json\nimport os\nimport pandas as pd"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"819fcc3e-bcad-4793-9cdf-87818c91f3df"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##Create a Data Hub client and specify data to pull"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"384aab3d-b768-4adc-99db-da169962fe2a"}}},{"cell_type":"code","source":["client = ADHClient(\n  'v1',\n  dbutils.secrets.get(scope = \"key-vault\", key = \"DatabricksTenantId\"),\n  'https://uswe.datahub.connect.aveva.com',\n  dbutils.secrets.get(scope = \"key-vault\", key = \"DatabricksClientId\"),\n  dbutils.secrets.get(scope = \"key-vault\", key = \"DatabricksclientSecret\"))\nnamespace_id = dbutils.secrets.get(scope = \"key-vault\", key = \"DatabricksNamespaceId\")\n\ndata_view_id = 'Wind Turbine Analysis'\nstart_index = '2022-04-01T00:00:00.000Z'\nend_index = '2022-05-01T00:00:00.000Z'\ninterval = timedelta(minutes = 1)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c7369079-0c25-4c34-9c84-80a78f455120"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##Write function to pull data from Data View and generate a Spark Data Frame"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"87c65967-eff0-40ba-8a70-2dfbff40aaa9"}}},{"cell_type":"code","source":["def dataViewRequest(client, namespace_id, data_view_id, start_index, end_index, interval, cache):\n  data_page, next_page, first_page = client.DataViews.getDataInterpolated(\n    namespace_id, data_view_id, start_index=start_index, end_index=end_index, interval = interval, cache = cache)\n  df = spark.read.json(sc.parallelize([json.dumps(data_page)]))\n\n  # iterate through each subsequent page of results until there are no more pages\n  while next_page:\n    data_page, next_page, first_page = client.DataViews.getDataInterpolated(url=next_page)\n    df = df.union(spark.read.json(sc.parallelize([json.dumps(data_page)])))\n    \n  return df"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"53846325-9ba8-4f53-a5cf-ad1ea16d9279"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##Simple data request"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"985411a4-f055-4977-8db6-b6d332d6b1a1"}}},{"cell_type":"code","source":["df = dataViewRequest(client, namespace_id, data_view_id, start_index, end_index, interval, 'Refresh')\nprint(df.count())\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"19f88038-1995-4af9-81a5-23d506f9cbba"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##Parallel data request"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b92cc356-010e-4ef7-a8db-7f883bc8894e"}}},{"cell_type":"code","source":["# split time range from start_index to end_index into a list of intervals equal to the number of parallel executors\nnumber_of_executors = min(32, os.cpu_count() + 4)\n\n# split time range from start_index to end_index into a list of intervals equal to the number of parallel executors\nintervals = pd.date_range(start=start_index, end=end_index, freq=interval)\nintervals = intervals[0::round(len(intervals)/(number_of_executors+1))]\nprint(intervals)\n\n# convert the list of intervals to a list of start_indexes and end_indexes\nstart_indexes = intervals[:-1].tolist()\nstart_indexes = [i.strftime(\"%Y-%m-%dT%H:%M:%S\") for i in start_indexes]\nend_indexes = intervals[1:].tolist()\nend_indexes = [(i - interval).strftime(\"%Y-%m-%dT%H:%M:%S\") for i in end_indexes]\nend_indexes[-1] = end_index\n\nwith ThreadPoolExecutor(max_workers=number_of_executors) as pool:\n  # for each start_index and end_index, start a thread to pull data into a data_frame\n  data_frames = pool.map(dataViewRequest, repeat(client), repeat(namespace_id), repeat(data_view_id), start_indexes, end_indexes, repeat(interval), repeat('Refresh'))\n  \n  # combine all the data_frames into one\n  df = next(data_frames)\n  for data_frame in data_frames:\n    df = df.union(data_frame)\n    \nprint(df.count())\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"03349bdf-adfc-4dde-8484-b9f331392ac0"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Data Hub Data","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":1613947921670944}},"nbformat":4,"nbformat_minor":0}
